[ { "title": "The Solar Powered Kubernetes Cluster", "url": "/posts/solar-powered-kubernetes-cluster/", "categories": "", "tags": "", "date": "2023-08-14 00:00:00 -0600", "snippet": "The Solar Powered Kubernetes ClusterThis post is the first part in a series.The purpose of this series is to show that on-prem/private cloud/self-hosted kubernetes doesnâ€™t have to be hard and you ...", "content": "The Solar Powered Kubernetes ClusterThis post is the first part in a series.The purpose of this series is to show that on-prem/private cloud/self-hosted kubernetes doesnâ€™t have to be hard and you donâ€™t have to make too many compromises.Things that are hard in on-prem kubernetes Storage Networking LoadBalancers and by extension: Ingress Control Plane Access Cluster Autoscaling Bootstrapping new nodesThe Golden Rule of High AvailabilityOne is none and 3 is oneThis is an oversimplification, but for most high availability platforms, algorithms, and tools the minimum number of members you might want in a cluster is 3, as this allows the failure of one node while still allowing quorum. You will see this reflected in much of the architecture that follows.The hardware stackAt its core, the hardware stack is comprised of . Over the years, this NUC cluster has taken a few evolutions, such as a separate corosync network, but Iâ€™ve since simplified it.The solar setupRight now, my house has solar panels on its roof. I donâ€™t currently have whole home batteries, so part of the cluster runs on batteries, and the other part will power off when I am not producing solar in excess of demand. This is by design, to inject chaos into the cluster. Think of it like Spot instances on your cloud provider.Cluster Design PhilosophyThis cluster follows a gitops philosophy. This means that the application state will be completely driven by what you see in git. There may also be some gitops-style terraform run from argo workflows." }, { "title": "Build a docker image with Argo Workflows", "url": "/posts/build-docker-image-argo-workflows/", "categories": "", "tags": "", "date": "2023-02-26 00:00:00 -0700", "snippet": "Building a docker image with Argo WorkflowsIn this post, we will learn how to build a docker image using Argo Workflows, Kaniko, scan it with Trivy, and push it to an OCI Image repository.Using Arg...", "content": "Building a docker image with Argo WorkflowsIn this post, we will learn how to build a docker image using Argo Workflows, Kaniko, scan it with Trivy, and push it to an OCI Image repository.Using Argo Workflows for Continuous IntegrationOne of the common responsibilities of a DevOps Engineer is maintaining CI/CD for their company. There are many tools with various tradeoffs. Some are free, others are paid, some are fully integrated with your VCS (hopefully git). Jenkins is the venerable beast most have used at some point. Many of the newer tools are using YAML to abstract away much of what would have been done in groovy with Jenkins. Some examples of these tools in no particular order are Gitlab Runners, Github Actions, and Drone CI. But what if you want to do CI in a less-proprietary cloud-agnostic Kubernetes native way? Enter Argo Workflows.In the words of the Argo Project, Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows can be deployed the same, no matter where you run it. Whether you are in an airgapped on-prem Kubernetes cluster, or a managed Kubernetes cluster with a cloud provider, you can use Argo Workflows the same way. It also recently graduated ðŸŽ“ðŸŽ‰ as a CNCF Project.Key features of Argo Workflows Vendor and Cloud Agnostic Directed Acyclic Graphs Kubernetes Native Implemented as CRDs Can be used for many things, like Data Science or Infrastructure as Code implementationWTF is a Directed Acyclic Graph?A Directed Acyclic Graph or DAG, is a way to define steps or tasks inside of a workflow by declaring its dependencies, as opposed to declaring an explicit order. By doing this, you enable the workflow scheduler to achieve maximum parallelism without having to worry about factors such as the execution time of each individual step.PrerequisitesThis tutorial starts off by assuming you have a working install of Argo Workflows and can access the UI. If you donâ€™t have that, you can follow the official documentation. For this tutorial you do not need to do anything like configuring artifact repositories. You will also need a working Kubeconfig for the cluster that you will be developing, as well as have configured the Argo CLI.A typical CI WorkflowA typical Continuous Integration workflow looks something like this:1. Checkout Code2. Build Code3. Run Tests4. Run Security and Vulnerability Scanning5. Publish/Release CodeHow can we achieve this in Argo Workflows?---apiVersion: argoproj.io/v1alpha1kind: WorkflowTemplatemetadata: name: ci-templatespec: entrypoint: ci artifactGC: strategy: OnWorkflowDeletion volumeClaimTemplates: - metadata: name: workdir spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi arguments: parameters: - name: repo value: https://github.com/argoproj/argo-workflows.git - name: revision value: v2.1.1\" - name: oci-registry value: docker.io - name: oci-image value: templates/workflow - name: oci-tag value: v1.1.1 - name: push-image value: false templates: - name: ci dag: tasks: - name: git-clone template: git-clone - name: ls template: ls dependencies: - git-clone - name: build template: build dependencies: - git-clone - ls - name: trivy-image-scan template: trivy-image-scan dependencies: - build - name: trivy-filesystem-scan template: trivy-filesystem-scan dependencies: - build - name: push-image template: push-image when: \" == true\" dependencies: - trivy-filesystem-scan - trivy-image-scan - name: git-clone inputs: parameters: - name: repo value: \"\" - name: revision value: \"\" artifacts: - name: argo-source path: /src git: repo: \"\" revision: \"\" container: image: alpine:3.17 command: - sh - -c args: - cp -r /src/* . workingDir: /workdir volumeMounts: - name: workdir mountPath: /workdir - name: ls container: image: alpine:3.17 command: - sh - -c args: - ls / workingDir: /workdir volumeMounts: - name: workdir mountPath: /workdir - name: build inputs: parameters: - name: oci-image value: \"\" - name: oci-tag value: \"\" container: image: gcr.io/kaniko-project/executor:latest args: - --context=/workdir - --destination=: - --no-push - --tar-path=/workdir/.tar workingDir: /workdir volumeMounts: - name: workdir mountPath: /workdir - name: trivy-image-scan inputs: parameters: - name: oci-tag value: \"\" container: image: aquasec/trivy args: - image - --input=/workdir/.tar env: - name: DOCKER_HOST value: tcp://127.0.0.1:2375 volumeMounts: - name: workdir mountPath: /workdir sidecars: - name: dind image: docker:23.0.1-dind command: - dockerd-entrypoint.sh env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true mirrorVolumeMounts: true - name: trivy-filesystem-scan inputs: parameters: - name: oci-tag value: \"\" container: image: aquasec/trivy args: - filesystem - /workdir - --ignorefile=/workdir/.tar volumeMounts: - name: workdir mountPath: /workdir - name: push-image inputs: parameters: - name: oci-tag value: \"\" - name: oci-image value: \"\" - name: oci-registry value: \"\" script: image: gcr.io/go-containerregistry/crane:debug env: - name: OCI_REGISTRY_USER valueFrom: secretKeyRef: name: registry-credentials key: username - name: OCI_REGISTRY_PASSWORD valueFrom: secretKeyRef: name: registry-credentials key: password command: - sh source: &gt; crane auth login -u $OCI_REGISTRY_USER -p $OCI_REGISTRY_PASSWORD crane push /workdir/.tar /: volumeMounts: - name: workdir mountPath: /workdirThe BreakdownapiVersion: argoproj.io/v1alpha1kind: WorkflowTemplatemetadata: name: ci-templatespec: entrypoint: ci #Specifies what task the workflow will start on by default artifactGC: strategy: OnWorkflowDeletion # tells argo to cleanup after itself volumeClaimTemplates: - metadata: name: workdir spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi arguments: parameters: - name: repo value: https://github.com/argoproj/argo-workflows.git - name: revision value: v2.1.1\" - name: oci-registry value: docker.io - name: oci-image value: templates/workflow - name: oci-tag value: v1.1.1 - name: push-image value: falseThe VolumeClaimTemplate here is the notable part of this workflow. Instead of configuring an artifact repository, Argo Workflows has a facility to allow for seamlessly mounting a volume across individual steps or tasks in the workflow. This is extremely useful, because each task runs in its own ephemeral container. It also allows the re-use of assets between tasks without having to upload or download from somewhere else, as this is my biggest gripe with artifact caching in Github Actions.Below that, I specify workflow level parameters and their default values. These will be the defaults shown in the UI, and can be overridden there or in the on the command line. templates: - name: ci dag: tasks: - name: git-clone template: git-clone - name: ls template: ls dependencies: - git-clone - name: build template: build dependencies: - git-clone - ls - name: trivy-image-scan template: trivy-image-scan dependencies: - build - name: trivy-filesystem-scan template: trivy-filesystem-scan dependencies: - build - name: push-image template: push-image when: \" == true\" dependencies: - trivy-filesystem-scan - trivy-image-scanAbove, under templates: is where the actual tasks are defined. You can see here that the first template is named ci, which matches the entrypoint specified earlier. That means that unless overridden, this step ci will be executed when you run the workflow.You can also see that this is a dag. I have listed out all the other templates, or tasks to be performed as well as their dependencies. When any task has all of its dependencies complete, it will immediately begin execution. In this workflow, it looks a bit like this: You can see that git-clone is the first step, followed by ls. build needs to wait for both of those steps, next up are the two trivy scans. trivy-image-scan depends on the build stage, as it is scanning the actual docker image. On the other hand, trivy-filesystem-scan can begin executing immediately after the git clone. Once both are complete push-image can begin execution, provided that workflow.parameters.push-image is set to true. - name: git-clone inputs: parameters: - name: repo value: \"\" - name: revision value: \"\" artifacts: - name: argo-source path: /src git: repo: \"\" revision: \"\" container: image: alpine:3.17 command: - sh - -c args: - cp -r /src/* . workingDir: /workdir volumeMounts: - name: workdir mountPath: /workdirThis clones the repo specified by the parameters above, as well as the branch or ref specified in revision. It then copies the downloaded source onto a volume mount so that it can be seamlessly passed between other tasks in this workflow.There is a step called ls that is mainly included just for log output and to show how simple tasks can be performed. - name: build inputs: parameters: - name: oci-image value: \"\" - name: oci-tag value: \"\" container: image: gcr.io/kaniko-project/executor:latest args: - --context=/workdir - --destination=: - --no-push - --tar-path=/workdir/.tar workingDir: /workdir volumeMounts: - name: workdir mountPath: /workdirIn this step, I use Kaniko to build the image. Kaniko is a tool for building OCI/Docker images inside of Kubernetes. It avoids many of the pitfalls of running Docker in Docker (or in this case Docker in containerd). As you can see, it also doesnâ€™t require root privilege. I also specify output to the volume mount as a tar file, so that it can be used in downstream tasks. - name: trivy-image-scan inputs: parameters: - name: oci-tag value: \"\" container: image: aquasec/trivy args: - image - --input=/workdir/.tar env: - name: DOCKER_HOST value: tcp://127.0.0.1:2375 volumeMounts: - name: workdir mountPath: /workdir sidecars: - name: dind image: docker:23.0.1-dind command: - dockerd-entrypoint.sh env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true mirrorVolumeMounts: trueIn this step, I use Trivy to scan the previously built docker image. Trivy requires Docker, so how are we going to do that inside of this workflow? You can see that in the main Trivy container I specify the standard docker environment variable of DOCKER_HOST I point this to localhost which refers to the sidecar running the official docker:23.0.1-dind image. You can see that this requires privileged access to execute successfully, one of the many issues with running Docker in Docker.Next, I run a Trivy filesystem scan of the cloned source repo. I will skip explaining this step. - name: push-image inputs: parameters: - name: oci-tag value: \"\" - name: oci-image value: \"\" - name: oci-registry value: \"\" script: image: gcr.io/go-containerregistry/crane:debug env: - name: OCI_REGISTRY_USER valueFrom: secretKeyRef: name: registry-credentials key: username - name: OCI_REGISTRY_PASSWORD valueFrom: secretKeyRef: name: registry-credentials key: password command: - sh source: &gt; crane auth login -u $OCI_REGISTRY_USER -p $OCI_REGISTRY_PASSWORD crane push /workdir/.tar /: volumeMounts: - name: workdir mountPath: /workdirFinally, I use crane to push the newly built and scanned image to an OCI or Docker registry. In this step, I use the source command to specify that I am executing an inline script instead of a command and arguments. I mount a secret into the environment and then reference those credentials as environment variables. You will see that it is also possible to reference inputs and parameters directly inside of an inline script.Submitting the workflow:From the UI, itâ€™s fairly straight forward. From the command line, it looks a bit like this: argo submit --from workflowtemplate/ci-template -p repo=\"https://github.com/mikenabhan/proxmox-kubernetes-cluster-autoscaler.git\" -p revision=\"main\" -p oci-image=\"mikenabhan/proxmox-kubernetes-cluster-autoscaler\" -p oci-tag=\"test-tag\" -p push-image=\"false\" --watchThatâ€™s it, youâ€™ve now built a docker image using Argo Workflows, Kaniko, and scanned it for security vulnerabilities using Trivy. All in a cloud-agnostic manner." }, { "title": "Making a Geiger Counter with ESPHome and Home Assistant", "url": "/posts/esphome-home-assistant-geiger-counter-esp8266/", "categories": "", "tags": "", "date": "2020-12-11 00:00:00 -0700", "snippet": "Making a Geiger Counter with ESPHome and Home AssistantAfter a discussion with a friend about ridiculous things to hook up to a Home Automation system, I figured it out.Hardware and PrerequisitesFo...", "content": "Making a Geiger Counter with ESPHome and Home AssistantAfter a discussion with a friend about ridiculous things to hook up to a Home Automation system, I figured it out.Hardware and PrerequisitesFor this project you will need the following: A Working Home Assistant Install ESPHome installed and available An ESP32 or ESP8266 A RadiationD-v1.1(CAJOE) Geiger CounterThe Geiger Counter counter can be found from $30 USD to $100 USD across aliexpress, amazon, and ebay. You can usually find unassembled versions for a little cheaper, but the board is relatively dense and its not much more expensive for an assembled version. Regardless of which version you choose, make sure you install your Geiger-MÃ¼eller Tube in the correct direction, polarity matters.Assembly and SetupOnce you get your Geiger Counter assembled according to the instructions it is time to test. You need to hook it up to power. It came with a handy little USB plug, but that didnâ€™t end up working for me. The device draws ~15mA of power which means that we can skip the outside power supplies and hook it right up to the ESP. I am using a NodeMCU ESP32-s.There are three pins on the left side of the board labeled GND, 5V, and Vin. GND is Ground and 5v is your 5v power input. Confusingly the pin labeled Vin is just your data pin; we will address this one later.If you are using a NodeMCU (ESP8266 or ESP32) you can hookup ground to any ground pin (I had issues with the device not booting if I used the bottom left pin on my ESP32 based NodeMCU.). Hook up 5v to the Vin pin on the bottom left of your NodeMCU. Then go ahead and plug USB in directly and the device should power up. Be careful because the GM Tube has a charge of around 500 volts.Testing the DeviceYou should see one light on the right side of the board light up solid. Itâ€™s normal to have some background radiation, so you should hear a click accompanied by the LED in the lower left of the board flashing 20-40 times per minute.Here is a video showing correct powered up operation.Note that this device only detects Beta and Gamma particles. It wonâ€™t work for those of you wanting to detect alpha sources like Radon Gas.Prepping the MicrocontrollerWe are going to be using ESPHome because it makes things easy. It allows you to define pins and parameters for code instead of learning Arduino. It lets you specify that information and automatically compiles.I am going to post code here, but you should really just checkout my GitHub.substitutions: devicename: geiger upper_devicename: Geiger Counteresphome: name: $devicename platform: ESP32 board: nodemcu-32swifi: ssid: !secret wifi_ssid password: !secret wifi_password ap: ssid: ${upper_devicename} Fallback Hotspot password: !secret fallback_password captive_portal:# Optional manual IP# manual_ip:# static_ip: 192.168.0.195# gateway: 192.168.0.1# subnet: 255.255.255.0# Enable logginglogger:# Enable Home Assistant APIapi: password: !secret api_passwordota: password: !secret ota_passwordswitch: - platform: restart name: ${upper_devicename} Restart text_sensor: - platform: wifi_info ip_address: name: ${upper_devicename} IP Address ssid: name: ${upper_devicename} Connected SSID bssid: name: ${upper_devicename} Connected BSSIDsensor: - platform: pulse_counter pin: GPIO36 name: ${upper_devicename} id: \"geiger_counter\" unit_of_measurement: 'CPM' on_raw_value: - sensor.template.publish: id: radiation_level state: !lambda 'return x / 153.8;' # this was what I got for my data sheet and it matched reasonably well with the background data that I have. Many people are using other values. - platform: template name: \"Radiation Level\" id: \"radiation_level\" unit_of_measurement: 'ÂµSv/h' icon: mdi:radioactive accuracy_decimals: 5 binary_sensor: - platform: template device_class: safety name: \"Radiation Warning\" # This doesn't necessarily represent a \"dangerous\" count, but one that is abnormally high lambda: |- if (id(geiger_counter).state &gt; 100) { // High Count. return true; } else { // Normal Count. return false; }Add that into your ESPHome and upload in your preferred manner. I like to have ESPHome compile the binary and download it to my local machine because my Home Assistant server is not available nearby. You can flash the bin file to your ESP with a tool like ESPHome flasher. I ended up having to use this instead of my normal method because of something to do with formatting the ESP partitions. Disconnect the power wire from the Geiger Counter during your flash just for safety. Something weird might happen because of the high voltages.Once you get this all set up, go ahead and hook the pin erroneously labeled Vin on the Geiger Counter to the data pin you specified in your ESPHome YAML file. I used GPIO 36. Once everything is hooked up, go ahead and reconnect the power wires, and plug your ESP device into power.Bringing it all into Home AssistantESPHome supports autodiscovery, so you should get a notification in Home Assistant to add the device. If not, go ahead and go to Configuration&gt;Integrations and hit the big + ADD INTEGRATION at the bottom right. If your network is setup properly, you can choose esphome and type in geiger.local for the host, if not go ahead and login to your router and find the geiger counterâ€™s IP address and type it in the host.Once you get this all sorted you should see three new entities in Home Assistant. sensor.geiger_counter sensor.radiation_level binary_sensor.radiation_warningGo ahead and add them to Lovelace. Right now I am being pretty simple about it.lovelace-gIn the a future post, I may share how I am working with this data. Creating a notification for high radiation levels Creating a history stats sensor to track average, and high/low levels A node red flow that gets this data as well as the location of the sensor, and shares it with the Safecast API.Disclaimer: some of the product links may be affiliate links. I havenâ€™t been contacted by any vendors or companies, I am just using them for encouragement to write more content." } ]
